# Chapter 1
# Terminology
Computer System: focus on the Hardware
Operating System: focus on the Software that manages the Hardware.
___
# Basic Elements
## Operating System
- Manage the hardware resources
- Provides a set of services to users
	- Ex: Create the file, Run program, Access Internet
- Manages secondary memory (USB, SSD) and I/O input (External devices: keyboards, mouse)
## A Computer's Basic Elements
- Processor
- Main Memory
- I/O Modules
- System Bus
## Processor
- Controls the operation of the computer and performs data processing functions.
- When only one processor exists, it is commonly referred to as the Central Processing Unit (CPU).
- Contains internal registers:
    - Memory address register (MAR)
    - Memory buffer register (MBR)
    - I/O address register
    - I/O buffer register
## Main Memory
- **Volatile** (RAM): Data is typically lost when power is removed.
- Referred to as real memory or primary memory.
- Consists of a set of locations defined by sequentially numbered addresses.
	- Contains either data or instructions.
- In contrast, the contents of secondary storage (like disk memory) are retained when the system is shut down
## I/O Modules
- Transfers data from external devices to the processor and memory, and vice versa 
- Moves data between the computer and the external environment such as:
    - Storage (e.g. hard drive)
    - Communications equipment
    - Terminals
- Contains internal buffers for temporarily storing data before they are sent on
- Specified by an I/O Address Register (I/OAR).
## System Bus
- Provides for communication among processors, main memory, and I/O modules.
## Top-Level View

![[Top-Level View.png]]

# Processor Register
- Faster and smaller than main memory.
- **User-visible registers**: Enable the programmer to minimize main memory references by optimizing register use.
    - May be referenced by machine language.
    - Available to all programs (application and system).
    - Types: data, address, condition code registers.
- **Control and status registers**: Used by the processor to control its operation and by privileged OS routines to control the execution of programs.
## Data and Address Registers
- **Data**: Often general purpose, but some may have restrictions.
- **Address**:
    - Index Register
    - Segment pointer
    - Stack pointer
## Control and Status Registers
- **Program counter (PC)**: Contains the address of an instruction to be fetched.
- **Instruction register (IR)**: Contains the instruction most recently fetched.
- **Program status word (PSW)**: Contains status information.
## Condition Codes
- Also called **flags**.
- Bits set by processor hardware as a result of operations.
- Intended for feedback (read-only) regarding the results of an instruction execution.
___
# Instruction Execution
- A program consists of a set of instructions stored in memory.
- **Two steps**:
    1. Processor reads (fetches) instructions from memory.
    2. Processor executes each instruction.
## Basic Instruction Cycle
1.  **Fetch Stage**: Fetch the next instruction.
2.  **Execute Stage**: Execute the instruction.
![[Basic Instruction Cycle.png]]

## Instruction Fetch and Execute
- The processor fetches the instruction from memory.
- The Program Counter holds the address of the next instruction to be fetched and is incremented after each fetch.
- The fetched instruction is loaded into the Instruction Register (IR).
- Instruction categories: Processor-memory, processor-I/O, data processing, control.
___
# Interrupt 
- Interrupt the normal sequencing of the processor
- Provided to improve processor utilisation
	- Most IO devices are slower than the processor
	- The processor must pause to wait for the device
- Example: divide by 0, infinite loop
## Common Classes of Interrupts
| Class                | Description                                                                            |
| :------------------- | :------------------------------------------------------------------------------------- |
| **Program**          | Generated by an instruction execution error (e.g., overflow, division by zero).        |
| **Timer**            | Generated by an internal processor timer to allow the OS to perform regular functions. |
| **I/O**              | Generated by an I/O controller to signal completion or an err_or.                      |
| **Hardware failure** | Generated by a failure like a power outage or memory parity error.                     |
## Instruction Cycle with Interrupts
- The processor checks for interrupts after executing an instruction.
- If an interrupt is pending, it suspends the current program and executes an interrupt handler.
![[Instruction Cycle with Interrupts.png]]
## Multiple Interrupts
- Suppose an interrupt occurs while another interrupt is being processed.
- Two approaches:
    1.  **Disable interrupts** during interrupt processing (Sequential Interrupt Processing).
    2.  Use a **priority scheme** to handle higher-priority interrupts (Nested Interrupt Processing).
## Multiprogramming
- The processor has more than one program to execute.
- The sequence of execution depends on relative priority and whether programs are waiting for I/O.
- After an interrupt handler completes, control may not return to the program that was executing at the time of the interrupt.
___
# The Memory Hierarchy
- Major constraints in memory: Amount, Speed, Expense.
- Trade-offs:
    - Faster access time, greater cost per bit.
    - Greater capacity, smaller cost per bit.
    - Greater capacity, slower access speed.
## Hierarchy Levels
- Going down the hierarchy:
    - Decreasing cost per bit
    - Increasing capacity
    - Increasing access time
    - Decreasing frequency of access to the memory by the processor
___
# Cache Memory
- Invisible to the OS; interacts with other memory management hardware.
- Exploits the **principle of locality** with a small, fast memory to improve performance.
- **Principle of Locality**: Data required soon is often located near the currently accessed data.
## Cache Principles
- Contains a copy of a portion of main memory.
- The processor first checks the cache for data.
- If the data is not found (a miss), a block of memory is read from main memory into the cache.
- Due to locality, future memory references are likely to be in that block.
## Cache Design Issues
- **Cache size**: Small caches still have a significant impact.
- **Block size**: The unit of data exchanged. Larger blocks mean more hits but can reduce the chance of reuse.
- **Mapping function**: Determines which cache location a block will occupy.
- **Replacement algorithm**: Chooses which block to replace when a new block is loaded (e.g., Least Recently Used - LRU).
- **Write policy**: Dictates when a memory write operation takes place.
___
# I/O Communication Techniques

## Three Techniques
1.  **Programmed I/O**: The processor must periodically check the status of the I/O module. The processor waits for the I/O operation to complete.
2.  **Interrupt-driven I/O**: The processor issues an I/O command and continues with other work. The I/O module interrupts the processor when it is ready.
3.  **Direct memory access (DMA)**: An I/O operation is delegated to a separate DMA module. The processor is only involved at the beginning and end of the transfer, allowing it to perform other tasks. DMA is much more efficient.
___
# Notes
- **Concurrency** = multiple tasks _in progress_ (even if not literally at the same instant).
- **Parallelism** = multiple tasks _running at the same time_ (requires numerous cores/CPUs).

# Chapter 2
# Operating System
- A program that controls the execution of application programs.
- **OS as User/Computer Interface** The OS masks the details of the hardware from the programmer, providing a convenient interface and acting as a mediator for application programs to access facilities and services
- **OS as Resource Manager** The OS is responsible for controlling the use of a computer’s resources, including I/O, main and secondary memory, and processor execution time.
- **Goals**:
    - **Convenience**: Makes the computer more convenient to use.
    - **Efficiency**: Allows computer system resources to be used in an efficient manner.
    - **Ability to evolve**: Permits effective development, testing, and introduction of new system functions without interfering with service.
## OS Functionality
- Functions the same way as other computer software; it is a program that is executed.
- The operating system relinquishes control of the processor to run other programs.
- **Kernel**: The heart of the operating system.
    - The portion of the operating system that is in main memory.
    - Contains the most-frequently used functions.
## Layers of Computer System
Application -> Utilities -> OS -> Computer Hardware
- Utilisation (middleware): Running on different OS -> environment, library, dependencies
## Operating System Services
- **Program development**: Editors, compilers, and debuggers.
- **Program execution**: Process and thread scheduling.
- **Access to I/O devices**.
- **Access to filesystem(s)**.
- **Access to network(s)**.
- **Accounting**: Monitor performance, used for billing.
- **Error detection and response**: Handles internal and external hardware and software errors.
___
# Uniprogramming vs. Multiprogramming
## Uniprogramming
- Single running process: no threads
- The processor must wait for an IO instruction to complete before proceeding.
## Multiprogramming
- Multitasking
- When one job needs to wait for I/O, the processor can switch to the other job.
## Time Sharing
Using multiprogramming to handle multiple interactive jobs (real-time applications): slices the time into many slots to execute concurrently
 - The processor’s time is shared among multiple users.
 - Multiple user processes simultaneously access the system
> Key objective is to **minimize response time**

|                          | Batch Multiprogramming        | Time Sharing                  |
| :----------------------- | :---------------------------- | :---------------------------- |
| **Principal objective**  | Maximize processor use        | Minimize response time        |
| **Source of directives** | Job control language commands | Commands entered in real time |
___
# Processes
Several definitions of a process:
- **A program in execution in the main memory**
- An instance of a program *in execution*.
- The entity that can be assigned to and executed on a processor
- A unit of activity characterised by a single sequential thread of execution, a current state, and an associated set of system resources
Consists of three components:
1. An executable program
2. Data needed by the program: static data
3. Execution context of the program: all information the OS needs to manage the process
- Context: ID, attribute
- All the processes are in the stack
## Memory Management
- **Process isolation**: Prevent independent processes from interfering with each other.
- **Automatic allocation and management**: Dynamic allocation across the memory hierarchy.
- **Support for modular programming**.
- **Protection & access control**: Private & shared memory areas. 
- **Long-term storage** (non-volatile).
### Virtual Memory
- Allows programmers to address memory from a logical point of view (Virtual RAM: max is double the physical RAM)
- Processes move between primary and secondary memory (using the hard disk to act as the RAM)
- **Paging**:
	- Allows the process to be comprised of several fixed-size blocks, called **pages** 
		- Segment the code into different modules)
	- A virtual address is a page number and an offset within the page
	- Each page may be located anywhere in main memory
	- The real address or physical address in main memory
![[Virtual Memory.png]]
## Scheduling and Resource Management
**Scheduling**: manage the process within the time slot
**Resource Management**: manage the resources for the process
- **Fairness**: Give equal and fair access to all processes.
- **Differential responsiveness**: Discriminate between different classes of jobs
	- High/low Priority: more resources
	- User/System
- **Efficiency**: Maximize throughput, minimize response time, and accommodate as many users as possible.
	- Balance resources and CPU time
## System Structure
- View the system as a series of levels (App → OS → Hardware)
- Each level performs a related subset of functions
- Each level relies on the next lower level to perform more primitive functions
- This decomposes a problem into several more manageable sub-problems
## Operating System Design Hierarchy
H: high level, I: intermediate level, L: low level

| Level | Objects                                     | Level | Objects                                    |
| ----- | ------------------------------------------- | ----- | ------------------------------------------ |
| 6I    | Blocks of data, device channels             | 13H   | User programming environment               |
| 5I    | Primitive process, semaphores, ready list   | 12H   | User processes                             |
| 4L    | Interrupt-handling programs                 | 11H   | Directories                                |
| 3L    | Procedures, call stack, display             | 10H   | External devices (printer, displays, etc.) |
| 2L    | Evaluation stack, micro-program interpreter | 9H    | Files                                      |
| 1L    | Registers, gates, buses, etc.               | 8H    | Pipes                                      |
|       |                                             | 7I    | Segments, pages                            |
# Modern OS
## Modern Operating Systems
- **Symmetric multiprocessing (SMP)**: More than one processor, all processors are equal and share memory/I/O.
- **Distributed operating systems**: Provides the illusion of a single memory system across multiple machines.
- **Object-oriented design**: For adding modular extensions to a small kernel.
### Mircokernel architecture
- As opposed to a monolithic kernel, it assigns only a few essential functions to the kernel:
	- Address space management.
	- Interprocess Communication (IPC): communicate between processes, create a process
	- Basic Scheduling
-  **Threads**: A process is divided into a plurality of concurrent threads.
___
# UNIX
- Hardware is surrounded by the operating-system kernel.
- Comes with user services and interfaces (C compiler, shell, etc.).
- **Modern Unices**: System V (SVR4), BSD, Solaris, Linux, macOS.

# Chapter 3 
**Process states** that characterise the behaviour of processes.
**Data structures** used to manage processes.
___
# Process Representation and Control
## Requirements of OS
- **Fundamental Task: Process Management** - Interleaving

![[Multiprogramming.png]]

The OS must:
- **Interleave** the execution of multiple processes: maximise the CPU power
- Allocate resources to processes (the process is always greedy), and protect the resources of each process from other processes. ⇒ Optimise resources (too many resources in a specific process waste resources) and the time
- Enable Processes to share and exchange information.
- Enable synchronisation among processes
## The OS manages the Execution of Applications
 - Resources are made available to multiple applications.
- The processor is switched among multiple applications.
- The processor and I/O devices can be used efficiently.
## What is a Process? ([[Chapter 2 - Operating System Overview]])
## Process Elements
- A process is comprised of:
    - Program code (possibly shared)
	- Set of data: defined in the code: declare the variables (fixed-size data), or a static array
	- Number of attributes describing the state of the process
- While the process is running, it has several elements, including:
	- ID (identifier): can be reused (1→ 1000: system) (10000→ : user)
		- Example: HTTP process: ID + port
	- State: Let OS know it's running, ...
	- Priority: Know the priority 
	- PC
	- Memory Pointers
	- Context data
	- IO status information
	- Accounting information
## Process Control Block (PCB)
> **Always load to the main memory - first steps need to do**
- Contains the process elements
- Create and manage by OS
- Allows support for multiple processes
## Trace of the Process
- The behavior of an individual process is shown by listing the sequence of instructions that are executed. This list is called a **Trace**.
- **Dispatcher**: A small program that switches the processor from one process to another.
___
# Process States

## Two-States Process Model
![[Two-state Process Model.png]]
- A process may be in one of two states:
    - **Running**
    - **Not-running**
- Ex: When you double-click to run the app: The app will go to the state Not-running state (check the resources that are enough to run - if not, stop immediately).
## Queuing Diagram
![[Queue Diagram.png]]
When you click the app, the app will go into the ready queue, and wait to be processor execute
## Process Birth and Death

| **Reasons for Process Creation**       | **Reasons for Process Termination** |
| ---------------------------------- | ------------------------------- |
| New batch job                      | Normal Completion               |
| Interactive Login                  | Memory unavailable              |
| Created by OS to provide a service | Protection error                |
| Spawned by an existing process     | Operator or OS Intervention     |
## Process Creation
- The OS builds a data structure to manage the process
- Traditionally, the OS created all processes: But it can be useful to let a running process create another
- This action is called **process spawning**
	- **Parent Process** is the original, creating process
	- **Child Process** is the new process
	- Parents can keep connected with the child to synchronise. If the OS kill the parent, the child becomes an **orphan process**
	- **Zombie Process**: Cannot delete data, or the process does not clean up all the data.
## Process Termination
- There must be some way that a process can indicate completion. 
- This indication may be:
	- A HALT instruction generating an interrupt alert to the OS. 
	- A user action (e.g. log off, quitting an application)
	- A fault or error 
	- Parent process terminating
## Five-State model

![[Five-State Process Model.png]]
- **New**: The process is being created.
- **Ready**: The process is waiting to be assigned to a processor.
- **Running**: Instructions are being executed.
- **Blocked**: The process is waiting for some event to occur.
- **Exit**: The process has finished execution.
## Using Two Queues
![[Two Queues.png]]
![[Multiple Blocked Queue.png]]
## Suspended Process
- The processor is faster than the I/O, so all processes could be waiting for I/O
	- *Swap these processes to disk to free up more memory and use the processor on more processes.*
- The blocked state becomes the **suspended state** when swapped to disk
- Two new states
	- **Blocked/Suspend**: wait in the secondary memory to get data
	- **Ready/Suspend**: finish and still in the secondary memory
![[Suspended Process.png]]
- New → Ready Suspend: The User wants to run the program, the OS tries to run that. All the conditions are satisfied, but it does not have enough main memory. It will run it in the Virtual Memory, the OS believes that the RAM will be free up soon
- Ready → Ready Suspend: New process is a higher priority than the queue process
- Running → Ready Suspend: Process consumes a part of memory. When the CPU processes, it may split the process into two different parts and put them in different types of memory. Maybe the process in the fast memory requests the data in the slow memory, to the CPU kicks it out to the Ready Suspended.
## Reason For Process Suspension

| **Reason**               | **Comment**                                                                                                                                                      |
| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Swapping**                 | The OS needs to release sufficient main memory to bring in a process that is ready to execute.                                                                   |
| **Other OS Reason**          | OS suspects process of causing a problem.                                                                                                                        |
| **Interactive User Request** | e.g., debugging or in connection with the use of a resource.                                                                                                     |
| **Timing**                   | A process may be executed periodically (e.g., an accounting or system monitoring process) and may be suspended while waiting for the next time.                  |
| **Parent Process Request**   | A parent process may wish to suspend execution of a descendent to examine or modify the suspended process, or to coordinate the activity of various descendants. |
___
# Data Structures
## OS Control Structures
- For the OS to manage processes and resources, it must have information about the current status of each process and resource.
- Tables are constructed for each entity, and the operating system manages
	- **Memory Tables**: Track main and secondary memory.
    - **I/O Tables**: Manage I/O devices and channels.
    - **File Tables**: Information about files.
    - **Process Tables**: Manage processes. 
### Memory Tables
Memory tables are used to keep track of both main and secondary memory.
Must include this information:
- Allocation of main memory to processes
- Allocation of secondary memory to processes
- Protection attributes for access to shared memory regions
- Information needed to manage virtual memory
### I/O Tables
Used by the OS to manage the I/O devices and channels of the computer. The OS needs to know
- Whether the I/O device is available or assigned
- The status of the I/O operation
- The location in the main memory is being used as the source or destination of the I/O transfer
### File Tables
These tables provide information about:
- Existence of files
- Location on secondary memory
- Current Status
- other attributes.
Sometimes this information is maintained by a file management system
### Process Table
To manage processes, the OS needs to know details of the processes
- Current state
- Process ID
- Location in memory
- etc
Process control block: **Process image** is the collection of programs: Data, stack, and attributes
#### Process Control Block (PCB)
- The most important data structure in an OS.
- Contains all process elements and attributes.
- Defines the state of the OS.
- Requires protection, as a fault could destroy the OS's ability to manage the process.
- **Process Image**: The collection of program, data, stack, and attributes.
##### Information in a PCB
- **Process Identification**: Unique numeric ID, user IDs, etc.
- **Processor State Information**: Contents of processor registers (user-visible, control/status, stack pointers), PSW.
- **Process Control Information**: Scheduling info, memory management, resource ownership, etc.
###### Process identification
Each process is assigned a *unique numeric identifier.*
Many of the other tables controlled by the OS may use process identifiers to cross-reference process tables.
###### Processor State Information
This consists of the contents of the processor registers.
- User-visible registers
- Control and status registers
- Stack pointers
Program status word (PSW):
- contains status information
- Example: the EFLAGS register on Pentium processors
###### Process control information
 This is the additional information needed by the OS to control and coordinate the various active processes.
##### Roles of PCB
The *most important data structure in an OS*: 
- It defines the state of the OS
- OS is always protected PCB; if it is broken, the program becomes unstable.
Process Control Block requires protection:
- A faulty routine could cause damage to the block, destroying the OS’s ability to manage the process.
- Any design change to the block could affect many modules of the OS
---
# Ways in which the OS uses these data structures to control process execution
## Modes of Execution
- Most processors support at least two modes of execution:
- **User mode**: Less-privileged. User programs execute here.
	-  Normal code, such as $2+2=4$
- **System mode**: More-privileged. The kernel of the OS executes here.
	- Ex: Create a new process, the malloc() function, and use the computer resources
## Process Creation
- When the OS decides to create a new process (e.g., via `fork()`):
    1.  Assigns a *unique process identifier*: sometimes the ID can be reused
    2.  Allocates space for the process.
    3.  Initializes the process control block.
    4.  Sets up appropriate linkages (e.g., to parent process, address, stack, instruction)
    5.  Creates or expands other data structures:  I/O, File table.
## Switching Processes
Several design issues are raised regarding process switching
- What events trigger a process switch? The dispatcher and “time interrupt” do this 
- We must distinguish between mode switching and process switching.
- What must the OS do to the various data structures under its control to achieve a process switch?
>  A process switch may occur any time the OS has gained control from the currently running process. Possible events include:

| Mechanism           | Cause            | Use                                         |
| :------------------ | :--------------- | :------------------------------------------ |
| **Interrupt**       | External event   | Reaction to an asynchronous external event  |
| **Trap**            | Error/exception  | Handling of an error or exception condition |
| **Supervisor Call** | Explicit request | Call to an operating system function        |

## Change of the process state
- The steps in a process switch are:
	1. Save the context of the processor (program counter, registers).
	2. Update the PCB that is currently in the Running state.
	3. Move the PCB to the appropriate queue (ready, blocked, eady/suspend, .etc.).
	4. Select another process for execution.
    5. Update the PCB of the selected process.
    6. Update memory-management data structures.
    7. Restore the context of the selected process.
## Is the OS a Process?
- **Non-process Kernel**: The kernel executes outside of any process. OS code is executed as a separate entity in privileged mode.
- **Execution Within User Processes**: The OS executes software within the context of a user process.
- **Process-based Operating System**: The OS is implemented as a collection of system processes.
- If the OS is just a collection of programs and if it is executed by the processor just like any other program, is the OS a process? Yes, it is a special program which controls other programs. The OS have two parts: Kernels and Services
- If so, how is it controlled? Who (what) controls it? Kernel, Services, and Process Switching
### Non-Processor Kernel
- Execute kernel outside of any process 
- The concept of process is considered to apply only to user programs. Operating system code is executed as a separate entity that operates in privileged mode
### Execution within User Process
- Operating system software within context of a user process 
- No need for Process Switch to run OS routine
### Process-based operating system 
- Implement the OS as a collection of system process
## Security Issue
- An OS associates a set of privileges with each process (e.g., administrator/root).
- A key security issue is to prevent unauthorized privilege escalation.
- **System Access Threats**: Intruders (Masquerader, Misfeasor, Clandestine user) and Malicious software.
## System access threats
- Intruders
	- Masquerader (outsider)
	- Misfeasor (insider)
	- Clandestine user (outside or insider)
- Malicious software (malware)
### Countermeasures: Intrusion Detection
- Intrusion detection systems are typically designed to detect human intruders and malicious software behaviour.
- May be host or network-based
- Intrusion detection systems (IDS) typically comprise
	- Sensors
	- Analyzers
	- User Interface
### Countermeasures: Authentication
- Two Stages:
	- Identification
	- Verification
- Four Factors:
	- Something the individual **knows**
	- Something the individual **possesses**
	- Something the individual is **(static biometrics)**
	- Something the individual does **(dynamic biometrics)**
### Countermeasures: Access Control
- A policy governing access to resources
- A security administrator maintains an authorisation database
	- The access control function consults this to determine whether to grant access.
- An auditing function monitors and keeps a record of user accesses to system resources.
### Countermeasures: Firewalls
- Traditionally, a firewall is a dedicated computer that:
	- interfaces with computers outside a network
	- has special security precautions built into it to
	- Protect sensitive files on computers within the network.

# Chapter 4
# Threads: Resource ownership and execution
## Processes and Threads
**Processes** have two characteristics are treated independently by the OS:
- **Resource ownership:** unit of resource ownership: a **process**/**task**
	- **Characteristics**: A process owns the resources necessary for the application to run.
	- **Components**: Associated elements include the virtual address space (holding the process image, data, code, and attributes defined in the PCB), memory containing code and data, open files, devices, and I/O channels
- **Scheduling/execution:** Follow an execution path (the flow of the code - instruction in the codebase) that may be interleaved with other processes.

**The Thread (Unit of Execution and Dispatching)**: This is the fundamental unit that is actually scheduled and dispatched by the OS (**lightweight process**)
- **Characteristics**: A thread is an independent path of execution within a process.
- **Components**: Associated elements include a thread execution state (Running, Ready, Blocked), an execution stack, and a saved thread context when it is not running (containing the program counter and stack pointer)
## Multithreading
The ability of an OS to support multiple, concurrent paths of execution within a single process.
- **Single Thread Approaches**: MS-DOS supports a single process with a single thread. Some UNIX versions support multiple processes but only one thread per process.
- **Multiple Threads per Process**: Found in Java run-time environments, Windows, Solaris, and modern UNIX versions. 
	- Advantages: Lower Overhead, Efficient Communication:

![[Processes and Threads.png]]

## Processes ([[Chapter 2 - Operating System Overview]])
- A virtual address space that holds the process image
- Protected access to 
	- Processors
	- Other processes
	- Files
	- I/O resources
## Once or More Threads in Process
Each thread typically has:
- An execution state (running, ready, blocked, etc.)
- Saved thread context when not running
- An execution stack (process stack, including the user stack, stores dynamic state, and the system stack is allocated by the OS)
- Some per-thread static storage for local variables
- **Shared Access**: Access to the memory and resources of its process (all threads of a process share this

> A way to view a thread is as an independent program counter OS a process.
## Threads vs. Processes
![[Single Vs. Multi.png]]

- **Single-Threaded Process Model**: One Control Block managing one thread/execution path.
- **Multithreaded Process Model**: One Process Control Block, but multiple Thread Control Blocks manage parallel execution paths within the shared address space. These threads know each other by using the shared User Address Space.
### Benefits of Threads
- Takes less time to create a new thread than a process. Creating the process takes many states; the thread is easier.
- Less time to terminate a thread than to terminate a process. The process takes the resource by the OS, and the thread takes the resource by using the shared resource inside the process.
- Switching between two threads takes less time than switching processes.
- Threads can communicate with each other without invoking the kernel.
## Thread Use in a Single-User System
- Foreground and background work
- Asynchronous processing
- Speed of execution
- Modular program structure
- Remote procedure calls
Example: The web server opens port 80 to listen to the client. If the client connects to the server, the server blocks the port and creates a thread to handle that client. The server can create as many threads as it can handle, as many as clients. 
## Threads
Several actions that affect all of the threads in a process
- The OS must manage these at the process level
Examples:
- Suspending a process involves suspending all threads of the process.
- Termination of a process terminates all threads within the process.
## Activities Similar to Processes
- Threads have execution states, and many synchronize with one another.
- We look at these 2 aspects of thread functionality in turn.
	- States
	- Synchronization
## Thread Execution States

| State Change Activity | Description                                                  |
| :-------------------- | :----------------------------------------------------------- |
| **Spawn**             | Creates another thread.                                      |
| **Block**             | An issue will block a thread block other, or *all*, threads. |
| **Unblock**           |                                                              |
| **Finish (thread)**   | Deallocate register context and stacks.                      |
## Example: Remote Procedure Call
Consider:
- A program that performs 2 remote procedure calls (RPCs)
- to two different hosts
- To obtain a combined result.
## RPC Using a Single Thread
![[RPC Single Thread.png]]
## RPC Using One Thread per Server 
![[RPC Multithread.png]]
## Multithreading on a Uniprocessor

## Categories of Thread Implementation
- User Level Thread (ULT): managed by the app itself through a **threads library** in user space
- Kernel Level Thread (KLT), also called.
	- Kernel-supported threads
	- Lightweight processes
### User-Level Threads
- **Management:** The kernel (OS) is unaware of the existence of the threads within the process. The library contains the code for creating, destroying, scheduling, and saving/restoring thread contexts
## Relationships Between ULT Thread and Process States
![[Relationships Between ULT Thread and Process States.png]]
The picture describes the relationship between process B and the two self-threads 1, 2.  The thread can change the state when process B is running. Other state of the process B, it freezes the state of the Threads.
### Kernel-Level Threads
- KLTs are threads maintained and managed directly by the OS kernel.
	- **Management:** The kernel is aware of and maintains the context for every thread. No thread management done by application.
		- Scheduling is done on a thread basis
	- **Efficiency:** Switching between KLTs requires a **mode switch** to the kernel, making the switch slower than a ULT switch.
	- **Concurrency:** KLTs allow multiple threads from the same process to execute simultaneously (in parallel) on a multiprocessor or multicore system
	- Windows is an example of this approach
#### Advantages of KLT
- The kernel can simultaneously schedule multiple threads from the same process on multiple processors.
- If one KLT blocks, the kernel can simply schedule another ready thread (from the same process or a different one) for execution. The entire process is **not blocked**
- Kernel routines themselves can be multithreaded.
#### Disadvantages of KLT
The transfer of control from one thread to another within the same process requires a mode switch to the kernel.
## Combined Approaches
- Thread creation is done in the user space.
- The bulk of scheduling and synchronization of threads by the application 
- An example is Solaris.
## Relationship Between Threads and Processes

| Thread/Process Ratio | Description                                                                                                                                                                                            |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **1:1**              | Each thread of execution is a unique process with its own address space and resources. Traditional UNIX implementations                                                                                |
| **M:1**              | A process defines an address space and dynamic resource ownership. Multiple threads may be created and executed within that process (managed by a UTL). Windows NT, Solaris, Linux, OS/2, OS/390, MACH |
| **1:M**              | A thread may migrate from one process environment to another. This allows a thread to be easily moved among distinct systems.                                                                          |
| **M:N**              | Combines attributes of M:1 and 1:M cases. Ra (Clouds), Emerald, TRIX                                                                                                                                   |

___
# Symmetric Multiprocessing (SMP)
## Traditional View
Traditionally, the computer has been viewed as a sequential machine. 
- A processor executes instructions one at a time in sequence.
- Each instruction is a sequence of operations
Two popular approaches to providing parallelism 
- Symmetric MultiProcessors (SMPs)
- Clusters 
## Categories of Computer Systems
- **Single Instruction Single Data (SISD)**: Single processor executes one instruction stream on data in single memory.
- **Single Instruction Multiple Data (SIMD)**: Each instruction executes on a different set of data by different processors. (See Parallel Processor Architectures diagram on page 6).
- **Multiple Instruction Single Data (MISD)**: Sequence of data transmitted to processors, each executing a different instruction sequence (Never implemented).
- **Multiple Instruction Multiple Data (MIMD)**: Processors simultaneously execute different instruction sequences on different data sets.
## Parallel Processor Architectures
Cluster: Many supercomputers are using the visualization methods to create many VPS
- **SIMD**: Master/Slave or Symmetric Multiprocessing (SMP).
- **MIMD**: Shared-Memory (tightly coupled) or Distributed-Memory (loosely coupled).
## Symmetric Multiprocessing
- Kernel can execute on any processor: Allowing portions of the kernel to execute in parallel
- Typically, each processor does self-scheduling from the pool of available processes or threads
## Typical SMP Organization

## Multiprocessor OS Design Considerations
The key design issues include
- Simultaneous concurrent processes or threads
- Scheduling
- Synchronization
- Memory Management
- Reliability and Fault Tolerance
___
# Microkernel

## Multiprocessor OS Design Considerations
Key design issues include:
- Simultaneous concurrent processes or threads.
- Scheduling.
- Synchronization.
- Memory Management.
- Reliability and Fault Tolerance.
## Microkernel
- A microkernel is a small OS core that provides the foundation for modular extensions.
- Key theoretical questions involve how small the kernel must be and whether drivers must reside in user space.
- Big question is how small must a kernel be to qualify as a microkernel – Must drivers be in user space?
- In theory, this approach provides a high degree of flexibility and modularity. 
### Kernel Architecture 
#### Microkernel Design: Memory Management
- Low-level memory management: Mapping each virtual page to a physical page frame.
- Most memory management tasks occur in **user space**..
#### Microkernel Design: Interprocess Communication (IPC)
- Communication between processes or threads in a microkernel OS is via **messages**.
- A message includes: 
	- A header identifying sender/receiver.
	- A body containing data, a pointer to data, or control information.
#### Microkernel Design: I/O and interrupt management 
Within a microkernel, it is possible to handle hardware interrupts as messages and to include I/O ports in address spaces. 
	*A particular user-level process is assigned to the interrupt, and the kernel maintains the mapping.* 
### Benefits of a Microkernel Organization
- Uniform interfaces on requests made by a process.
- Extensibility, Flexibility, Portability, Reliability.
- Distributed System Support.
- Object Oriented Operating Systems.

# Chapter 5
# Principles of Concurrency
___
## Multiple Processes
 - Central to modern OS is managing multiple processes through:
    - Multiprogramming
    - Multiprocessing
    - Distributed Processing
- **Big Issue is Concurrency**: Managing the interaction of all these processes.
## Concurrency
Concurrency arises in: 
- Multiple applications: Sharing time 
- Structured applications: Extension of modular design 
- OS structure (OS implemented as a set of processes or threads)
## Key terms

| Term                 | Description                                                                                                                                                                                                                                                          |
| :------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **atomic operation** | A sequence of one or more statements that appears indivisible; no other process can see an intermediate state or interrupt the operation. No middle state: finish or finish                                                                                          |
| **critical section** | A section of code within a process that requires access to shared resources and must not be executed while another process is in a corresponding section. The process only stay there for a specific period of time                                                  |
| **deadlock**         | A situation where two or more processes are unable to proceed because each is waiting for the other. Ex: go into the memory, then go out, not do something.                                                                                                          |
| **livelock**         | A situation where two or more processes continuously change their states in response to changes in others without doing any useful work.                                                                                                                             |
| **mutual exclusion** | The requirement that when one process is in a critical section accessing shared resources, no other process may access any of those shared resources.                                                                                                                |
| **race condition**   | A situation where the final result depends on the relative timing of multiple threads/processes reading/writing a shared data item. Using race to detect who will go into the critical section when we have many processes that want to access the critical section. |
| **starvation**       | A situation where a runnable process is overlooked indefinitely by the scheduler.                                                                                                                                                                                    |

## Interleaving and Overlapping Processes
**Concurrency - Multi-tasking - multiprogramming - interleaving**: Two processes run at the same time, but when we specify a period of time, *only one is running*.
**Parallel - Multiprocessing - Overlapping**: Two processes run at the same time. When we specify a period of time, many processes are running.
### Difficulties of Concurrency
- **Sharing of global resources**: Safely managing access to shared global resources is difficult.
- **Optimal resource allocation**: It is difficult for the OS to manage the allocation of resources optimally.
- **Locating programming errors**: Errors are hard to locate because results are often non-deterministic and difficult to reproduce.
> This makes debugging concurrency-related bugs extremely difficult, as they are not always reproducible and may only occur under specific timing conditions.
### Enforce Single Access
If a rule enforces that only one process can enter the function (mutual exclusion), P1 runs, P2 blocks until P1 completes, then P2 resumes.
## Race Condition
A race condition occurs when:
- Multiple processes or threads read and write shared data items.
- The final result depends on the specific order of execution of the processes.
- The output depends on which process "wins the race" to modify the data last.
> For example, if one process reads a shared variable, is interrupted, and a second process modifies that same variable, the first process will resume its work with stale, incorrect data, leading to a corrupted result.
## Operating System Concerns
The OS must:
- Keep track of the various processes.
- Allocate and de-allocate resources (processor time, memory, files, I/O devices).
- Protect the data and resources of each process from interference by other processes.
- Ensure that the process execution results are independent of the processing speed.
## Process Interaction
![[Relationship.png]]
- Three-level interaction:
	- Process unaware of each other:
		- Word vs Facebook → Competition
	- Process indirectly aware of each other:
		- Word vs Excel → Collaboration
	- Processes are directly aware of each other
		- Parent and child process: you get first, and I can use it
Three main control problems arise from process competition:
1.  **Need for Mutual Exclusion**:
2.  **Deadlock**
3.  **Starvation**
## Requirements for Mutual Exclusion
1. Only **one process** can be in its **critical section at a time** for a resource 
2. A process halted in its noncritical section must not interfere with others.
3. It must not be possible for a process requiring access to a critical section to be delayed indefinitely (no deadlock or starvation).
4. When no process is in a critical section, any process that requests entry must be permitted to enter without delay.
5. A process must not be delayed access to a critical section if no one else is using it.
	1. The OS treats all processes the same, regardless of size, speed, or big computation.
	2. They have the same limited time in the critical section.
6.  A process remains inside its critical section for a finite time only.
	1. The finite time used to lock other processes 
	2. Ex: Account back (money = critical section) and transaction = processes
		- If we have parallel transactions, withdraw and transfer money. 
		- The bank system will lock the money and execute transactions.
		- The back system will check every time it execute the transactions
---
# Mutual Exclusion: Hardware Support
## Disabling Interrupts
- Works only on **Uniprocessors**.
- Mutual exclusion can be guaranteed by disabling interrupts. A process will continue to run without being preempted until it enables interrupts again.
	- Example: The process interrupts in the critical section 
``` C
while (true) {
/* disable interrupts */;
/* critical section */;
/* enable interrupts */;
/* remainder */;
}
```
- **Major Drawbacks:**
	- System efficiency degrades because the processor is limited in its ability to interleave processes.
	- **This approach does not work on multiprocessor systems.** Disabling interrupts on one processor does not prevent other processors from accessing the shared resource.
## Special Machine Instructions
- These are hardware instructions that are performed as a single atomic operation, meaning they are not subject to interference from other processes.
- **Compare&Swap (CAS)**: Atomically compares the content of a memory location to a given value and, only if they are the same, modifies the content of that memory location to a new given value.
``` C
// The function below is an atomic operation
int compare_and_swap (int *word, int testval, int newval) {
	int oldval;
	oldval = *word
	if (oldval == testval) *word = newval;
	return oldval;
}
/* program mutualexclusion */
const int n = /* number of processes */;
int bolt;
void P(int i) {
	while (true) {
		while (compare_and_swap(bolt, 0, 1) == 1) //Race begin
		/* do nothing */;
		/* critical section */;
		bolt = 0; //Restart race
		/* remainder */;
	}
}

void main() {
	bolt = 0;
	parbegin (P(1), P(2), ... ,P(n));
}
```
- **Exchange**: Atomically swaps the contents of a register and a memory location.
``` C
// The function below is an atomic operation
void exchange (int *register, int *memory) {
	int temp;
	temp = *memory;
	*memory = *register;
	*register = temp;
}
/* program mutualexclusion */

int const n = /* number of processes */;
int bolt;
void P(int i) {
	while (true)
		int keyi = 1;
		do exchange (&keyi, &bolt) // Race begin
		while (keyi != 0);
		/* critical section */
		bolt = 0; //Restart race
		/* remainder */
	}

}

void main() {
	bolt = 0;
	parbegin (P(1), P(2), ..., P(n));
}
```
## Hardware Mutual Exclusion: Advantages & Disadvantages
- **Advantages**: Applicable to any number of processes (single or multiprocessor), simple to verify, supports multiple critical sections.
- **Disadvantages**:
	- **Busy-waiting**: A process repeatedly checks a condition in a loop, consuming processor time without performing useful work.
	- **Starvation** is possible because the selection of the next process to enter the critical section is not necessarily fair.
	- **Deadlock** is possible: multiple resources 
# Semaphores
---
## Semaphore (Data Structure)
- A semaphore is an integer value used for signaling among processes. Operations must be **atomic**: `initialize`, `Decrement (semwait)`, and `increment (semSignal)`.
	- **Binary Semaphore**: Value is restricted to zero or one (used like a lock).
	- **Strong Semaphores**
	- **Weak Semaphores
- It’s the number of resource values, it can be negative:
	- Example: I have 3 resources, a requests consume 3 → it becomes 0, and the second requests consume 2 → it becomes -2 (but it means that we do not have -2, but we need to wait 2 resources to execute the requests in the queue)
- Only three **atomic** operations are permitted on a semaphore:
	1. **Initialize**: Set the semaphore to a non-negative value.
	2.  **semWait (or P)**: Decrements the semaphore value. If the value becomes negative, the process executing the `semWait` is blocked. 
		1. It is a race.
	3.  **semSignal (or V)**: Increments the semaphore value. If the value is not positive, a process blocked by a `semWait` operation is unblocked.
``` C
struct semaphore {
	int count;
	queueType queue;
};
void semWait(semaphore s) { 
	s.count--;
	if (s.count < 0) { // We dont have enough resources
		/* place this process in s.queue */;
		/* block this process */;
	}
}
void semSignal(semaphore s) {
	s.count++;
	if (s.count<= 0) { // We don’t have enough resources, we have the process in the queue, we need to execute the queue first
	/* remove a process P from s.queue */;
	/* place process P on ready list */;
	}
}
```
## Strong/Weak Semaphore
- The distinction lies in how processes are unblocked from the semaphore's queue.
- **Weak Semaphore**: Does not specify the order in which processes are removed from the queue. (Random pick up, may have starvation)
- **Strong Semaphore**: Uses a **FIFO (First-In-First-Out)** policy, ensuring that the process that has been blocked the longest is released from the queue first. This prevents starvation.
## Mutual Exclusion Using Semaphores
- This is the most common application of semaphores.
- A semaphore (often called a mutex) is initialized to 1.
- A process performs a `semWait` operation before entering its critical section.
- The process performs a `semSignal` operation after exiting its critical section.
## The Producer/Consumer Problem
- **Scenario**: One or more **producers** generate data and place it in a shared buffer. A single **consumer** removes items from the buffer one at a time.
	- Example: Chef and customers in the Kichi Kichi
	- If the line is free, the Chef can put the sushi
	- The customer takes the food until the line is empty
- Constraints: 
	- Only one producer or consumer may access the buffer at any one time (Critical Section)
	- Ensure the producer can not add data to the full buffer, and the Consumer can not remove data from an empty buffer.
- **Challenges**:
	- The producer must not add data to the buffer if it is full.
	- The consumer must not remove data from the buffer if it is empty.
	- Only one process (producer or consumer) may access the buffer at any given time.
**Incorrect Solution**
``` C
/* program producerconsumer */
int n;
binary_semaphore s = 1, delay = 0;
void producer() {
	while (true) {
		produce();
		semWaitB(s);
		append();	
		n++;
		if (n==1) semSignalB(delay);
		semSignalB(s);
	}
}

void consumer() {
	semWaitB(delay); // Chặn comsumer vô critical section khi không có data = there is data in the buffer
	while (true) {
		semWaitB(s);	
		take();	
		n--;
		semSignalB(s);
		consume();
		if (n==0) semWaitB(delay); // Empty buffer
	}
}

void main() {
	n = 0;
	parbegin (producer, consumer);
}
```
![[Critical Section (Fail).png]]
Problem:
- n: global variable, it is a critical section, but we do not protect it.
Fix:
- Create the local variable to protect it:
## Bounded Buffer Problem Solution
- The solution for a finite buffer uses three semaphores:
	1.  `s`: A binary semaphore for enforcing **mutual exclusion** on buffer access, initialized to 1.
	2.  `n`: A counting semaphore to count the number of **items in the buffer**, initialized to 0.
	3.  `e`: A counting semaphore to count the number of **empty spaces in the buffer**, initialized to the buffer size.
- The producer waits on `e` before producing and signals `n` after. The consumer waits on `n` before consuming and signals `e` after. Both wrap their buffer access with waits and signals on `s`.
# Monitors
---
## Monitors
- A monitor is a programming language construct that provides equivalent functionality to semaphores but is easier to control.
- **Chief Characteristics**:
	- Local data variables are accessible only by the monitor's procedures.
	- A process enters the monitor by invoking one of its procedures.
	- **Only one process can execute in the monitor at a time**, providing implicit mutual exclusion.
## Synchronization in Monitors
- Synchronization is achieved using **condition variables** within the monitor.
- **cwait(c)**: Suspends the execution of the calling process on condition `c`. The monitor is now available for another process.
- **csignal(c)**: Resumes the execution of some process blocked on condition `c`. If no process is blocked, the signal has no effect.
# Message Passing
---
## Message Passing
- This is a mechanism for inter-process communication that synchronizes actions and exchanges data. It is an alternative to shared variables.
- It relies on a pair of primitives:
	- `send(destination, message)`
	- `receive(source, message)`
- This approach works on both shared-memory and distributed systems.
## Synchronization
- Primitives can be **blocking (synchronous)** or **non-blocking (asynchronous)**.
	- **Blocking send**: The sending process is blocked until the message is received.
	- **Blocking receive**: The receiving process is blocked until a message arrives.
	- A combination of blocking send and blocking receive is known as a **rendezvous**.
- **Non-blocking send**: The sending process continues execution immediately after sending.
- **Non-blocking receive**: The receiver retrieves a message if available, but does not wait if one is not.
## Addressing
- **Direct Addressing**: The `send` primitive explicitly identifies the destination process. The `receive` primitive can specify the desired source process.
	- You can not send it if the receiver does not accept
- **Indirect Addressing**:
	- Messages are sent to a shared data structure called a **mailbox**.
	- Processes communicate by sending messages to and receiving messages from the mailbox.
## Mutual Exclusion for Messages
``` C
/* program mutualexclusion */
const int n = /* number of process */
void P(int i) {
	message msg;
	while (true) {
		receive (box, msg);
		/* critical section */; // one receives message, others wait
		send (box, msg);
		/* remainder */; // Start new race
	}
}
void main() {
	create mailbox (box);
	send (box, null);
	parbegin (P(1), P(2), . . . , P(n)); 
}
```
# Readers/Writers Problem
---
## The Readers/Writers Problem
- **Scenario**: A data area is shared among many processes.
	- Some processes only read the data area (**Readers**).
	- Some processes only write to the data area (**Writers**).
- **Conditions to satisfy**:
	1.  Any number of readers may simultaneously read the file.
	2.  Only one writer at a time may write to the file.
	3.  If a writer is writing to the file, no reader may read it.
- There are two main variations of the problem based on priority:
	- **Readers have Priority**: No reader will be kept waiting unless a writer has already obtained permission to write. A waiting writer may be starved by a continuous stream of new readers.
	- **Writers have Priority**: Once a writer is ready to write, no new readers are allowed to start reading. A waiting reader may be starved by a continuous stream of new writers.
``` C
/* program readersandwriters */
int readcount;
semaphore x = 1,wsem = 1;
void reader(){
	while (true){
		semWait (x);
		readcount++;
		if(readcount == 1) semWait (wsem);
		semSignal (x);
		READUNIT();
		semWait (x);
		readcount--;
		if(readcount == 0) semSignal (wsem);
		semSignal (x);
	}
}
void writer() {
	while (true) {
		semWait (wsem);
		WRITEUNIT();
		semSignal (wsem);
	}
}
void main() {
	readcount = 0;
	parbegin (reader,writer);
}
```

# Chapter 6 
# Principles of Deadlock
---
## Deadlock
- A set of processes is **deadlocked** when each process in the set is blocked, awaiting an event that can only be triggered by another blocked process in the set. (*permanently blocked*)
- Typically involves processes competing for the same set of resources.
- There is **no efficient solution** to this problem.

> **Real-world analogy**: A four-way intersection where four cars arrive at the same time. Each car waits for the car on its right to proceed. No one can move. This is a deadlock.
> ![[Deadlock.png]]

**Example of Deadlock:**
![[Deadlock Example.png]]
**Deadlock-inevitable region** (**fatal region**): If paths of both P and Q go to this region, deadlock is definitely guaranteed. The reason is that at this moment, P has successfully gotten A and is ready to get B. Q has successfully gotten B and is ready to get A. (*P is going to ask for B to continue running, but Q is holding it. In addition, Q requires A, but at this time, it belongs to P*) 
-> Then, deadlock will happen since none of them can continue running.

**Example of no deadlock**
![[Example No Deadlock.png]]
## Resource Categories
Two general categories of resources:
- **Reusable Resources**: Can be used by only one process at a time and are not depleted by use. Examples include processors, memory, files, databases, and semaphores.
	- Deadlock occurs if each process holds one reusable resource and requests another. If the execution of 2 processes is $p_0,p_1,q_0,q_1,p_2,q_2$ -> deadlock occurs.
	  ![[Example of Two Processes Competing for Reusable Resources.png]]
	- Another example of deadlock with a reusable resource has to do with requests for main memory. Suppose the space available for allocation is 200 Kbytes, and the following sequence of requests occurs.
	  ![[Example of Two Processes Competing for Reusable Resources 2.png]]
- **Consumable Resources**: Can be created (produced) and destroyed (consumed). Examples include interrupts, signals, and messages. Typically, there is no limit on the number of consumable resources of a particular type.
	- Deadlock can occur if a `Receive` message is blocking and processes are waiting for messages from each other.
	  ![[Example of Two Processes Competing for Consumable Resources.png]]
## Resource Allocation Graphs
A directed graph that depicts a state of the system of resources and processes. 
![[Pasted image 20251010153150.png]]
Within a resource node, a dot is shown for each instance of that resource.

---
### Resource Allocation Graphs of Deadlock
![[Examples of Resource Allocation Graphs.png]]
- Figure (c): There is only one unit each of resources Ra and Rb. Process P1 holds Rb and requests Ra, while P2 holds Ra but requests Rb.
- Figure (d): Has the same topology, but there is no deadlock because multiple units of each resource are available.

Back to the example of 4 cars, here is the resource allocation graph for the deadlock case.
## Conditions for Possible Deadlock
For a deadlock to occur, four conditions must be met simultaneously:
1. **Mutual Exclusion**: Only one process may use a resource at a time.
2. **Hold-and-Wait**: A process may hold allocated resources while awaiting the assignment of others.
3. **No Preemption**: No resource can be forcibly removed from a process holding it. (no Priority) - No process force to get out of the queue.
4. **Circular Wait**: A closed chain of processes exists, such that each process holds at least one resource needed by the next process in the chain. (100 % has a deadlock).
## Dealing with Deadlock
There are three general approaches for dealing with deadlock:
1.  **Deadlock Prevention**: Design a system in such a way that the possibility of deadlock is excluded by negating one of the four necessary conditions.
2.  **Deadlock Avoidance**: Make a dynamic decision on whether the current resource allocation request will, if granted, potentially lead to a deadlock.
3.  **Deadlock Detection**: Grant resource requests whenever possible and periodically check for the presence of deadlock. If detected, perform recovery.
___
# Deadlock Prevention
---
## Deadlock Prevention Strategy
- **Indirect method**: Prevent one of the first three conditions at the same time (Mutual Exclusion, Hold-and-Wait, No Preemption).
- **Direct method**: Prevent the Circular Wait condition.
- **Attacking Mutual Exclusion**: Generally not possible, as some resources are inherently non-shareable. (Remove multitasking)
- **Attacking Hold-and-Wait**: Require a process to request all of its required resources at one time. This is inefficient as it may hold resources for a long time without using them. (Some processes need many different resources)
- **Attacking No Preemption**: If a process holding resources is denied a further request, it must release its original resources. 
- **Attacking Circular Wait**: Define a linear ordering of resource types. A process can only request resources in an increasing order of enumeration.

---
# Deadlock Avoidance
---
## Deadlock Avoidance Strategy
- A decision is made dynamically whether the current resource allocation request will, if granted, potentially lead to a deadlock. (simulation, if the OS grants the resources to the process, check if the system has a deadlock or not)
- This requires knowledge of future process resource requests.
- Two main approaches:
	- **Process Initiation Denial - New Process**: Do not start a process if its demands might lead to deadlock.
	- **Resource Allocation Denial - Existing Process (Banker's Algorithm)**: Do not grant an incremental resource request to a process if this allocation might lead to an unsafe state.
---
## Process Initiation Denial
A process is only started if the maximum claim of all current processes plus those of the new process, can be met. 

Consider a system of $n$ processes and $m$ different types of resources. Let us define the following vectors and matrices:
![[Process Initiation Denial.png]]
The matrix **Claim (C)** gives the maximum requirement of each process for each resource. The matrix **Allocation (A)** gives the current allocation to each process. The following relationships hold:
1.  $R_j = V_j + \sum_{i=1}^{n} A_{ij}$, for all j
    *   All resources are either available (V) or allocated (A).
2.  $C_{ij} \le R_j$, for all i, j
    *   No process can claim more than the total amount of resources in the system.
3.  $A_{ij} \le C_{ij}$, for all i, j
    *   No process is allocated more resources of any type than the process originally claimed to need.
With these quantities defined, we can define a deadlock avoidance policy. A new process $P_{n+1}$ is started only if:
$$R_j \ge C_{(n+1)j} + \sum_{i=1}^{n} C_{ij}, \quad \text{for all } j$$
That is, a process is only started if the maximum claim of all current processes plus the maximum claim of the new process can be met. This strategy is hardly optimal because it assumes the worst-case scenario: that all processes will make their maximum claims simultaneously.
Assumes the worst: that all processes will make their maximum claims together -> Not optimal.

---
## Banker's Algorithm (Resource Allocation Denial)
- State of the system is the current allocation of resources to process
- A state is **safe** if there is at least one sequence of process executions that allows all processes to run to completion.
- A state is **unsafe** if no such sequence exists. An unsafe state is not a deadlock, but it *may* lead to one.
- The Banker's Algorithm ensures the system never enters an unsafe state. When a request is made, the system pretends to grant it, then checks if the resulting state is safe. If it is, the request is granted; otherwise, the process must wait.

**Example:** The total amount of resources R1, R2, and R3 are 9, 3, and 6 units, respectively. In the current state allocations have been made to the four processes, leaving 1 unit of R2 and 1 unit of R3 available. Is this a safe state?
![[Banker's Algorithm.png]]

| Ký hiệu   | Tên bảng          | Ý nghĩa                                                     |
| --------- | ----------------- | ----------------------------------------------------------- |
| **C**     | Claim matrix      | Mức tài nguyên **tối đa** mà mỗi tiến trình có thể yêu cầu. |
| **A**     | Allocation matrix | Lượng tài nguyên **đang được cấp phát** cho mỗi tiến trình. |
| **C − A** | Need matrix       | Lượng tài nguyên **còn cần thêm** để tiến trình hoàn tất.   |
| **R**     | Resource vector   | Tổng số lượng tài nguyên hiện có trong hệ thống.            |
| **V**     | Available vector  | Số tài nguyên **hiện còn khả dụng** (chưa cấp phát cho ai). |
To answer this question, we ask an intermediate question: Can any of the four processes be run to completion with the resources available? That is, can the difference between the maximum requirement and current allocation for any process be met with the available resources? In terms of the matrices and vectors, the condition to be met for process i is:
$$C_{ij}-A_{ij}\leq V_j,\quad \text{for all j}$$
- This is not possible for P1, which has only 1 unit of R1 and requires 2 more units of R1, 2 units of R2, and 2 units of R3.
- However, if we assign one unit of R3 to process P2, then P2 has its maximum required resources allocated and can run to completion and return resources to ‘available’ pool

Can any of the remaining processes can be completed? **Note that P2 is completed**.
![[Banker's Algorithm 2.png]]
- Suppose we choose P1, allocate the required resources, complete P1, and return all of P1’s resources to the available pool.

**After P1 completed**
![[Banker's Algorithm 3.png]]
- Next, we can complete P3

**P3 completed**
![[Banker's Algorithm 4.png]]
- Thus, the state defined originally is a safe state.

**Another example:** This time we suppose that P1 makes the request for one additional unit each of R1 and R3. Is this safe?
![[Banker's Algorithm Deadlock.png]]
## Deadlock Avoidance
- When a process requests a set of resources
	- **Assume** that the request is granted
	- Update the system state accordingly
- Then determine if the result is a safe state. 
	- If safe, grant the request.
	- If not, block the process until it is safe to grant the request.
### Deadlock Avoidance Logic

### Deadlock Avoidance Advantages
- It is not necessary to preempt and rollback processes, as in deadlock detection
- It is less *restrictive* than deadlock prevention.
### Deadlock Avoidance Restrictions
- The *maximum resource requirement* must be stated in advance 
- Processes under consideration must be *independent* and with *no synchronization requirements*
- There must be a *fixed number of resources to allocate*
- No process may exit while holding resources
___
# Deadlock Detection
### Deadlock Detection Strategy
- ==Deadlock prevention and avoidance strategies can be very conservative==, limiting access to resources and restricting processes.
- ==Deadlock detection strategies are the opposite==:
	- They ***grant resource requests*** whenever possible.
	- The system ***does not check for deadlock before granting a resource***. Instead, the OS periodically **performs an algorithm to check for the presence of deadlock**.
### A Common Detection Algorithm
*   Use an Allocation matrix and an Available vector as previously.
*   Also, use a request matrix **Q**
    *   Where $Q_{ij}$ indicates that an amount of resource *j* is requested by process *i*.
*   First =='un-mark' all processes that are not deadlocked.== (mark is deadlock)
    *   Initially that is all processes.
### Detection Algorithm
1.  Mark each process that has a row in the Allocation matrix of all zeros. (Assume all the processes is deadlock)![[Deadlock Detection Algo.png]]
	1. Q: Need resources to finish
	2. A: Already allocated
	3. Resource Vector: resources we have
	4. W: Available vector:  (3) - A
2.  Initialize a temporary vector W to equal the Available vector.
3.  Find an index *i* such that process *i* is currently unmarked and the *i*th row of **Q** is less than or equal to **W**.
    *   i.e. $Q_{ik} \le W_k$ for $1 \le k \le m$.
    *   If no such row is found, terminate.
4.  If such a row is found,
    *   mark process *i* and add the corresponding row of the allocation matrix to **W**.
    *   i.e. set $W_k = W_k + A_{ik}$, for $1 \le k \le m$.
    *   Return to step 3.
*   A deadlock exists if and only if there are unmarked processes at the end.
*   Each unmarked process is deadlocked.
> ==Note:== Give conclusion deadlock or not, list the process is mark.
### Recovery Strategies Once Deadlock Detected
- Abort all deadlocked processes.
- Back up each deadlocked process to some previously defined checkpoint, and restart all process.
- Successively abort deadlocked processes until deadlock no longer exists. (delete one-by one until no deadlock).
- Successively preempt resources until deadlock no longer exists. (take back resources - only happen in priority system)

---
# Dining Philosophers Problem
### The Problem
- Devise a ritual (algorithm) that will allow the philosophers to eat.
- No two philosophers can use the same fork at the same time (mutual exclusion).
- No philosopher must starve to death (avoid deadlock and starvation ... literally!).
### Solutions Presented in the Slides
- **A first solution using semaphores**: A solution is presented where each philosopher waits on `fork[i]` and then `fork[(i+1) mod 5]`. This can lead to deadlock.
- **Avoiding deadlock**: A second solution is presented that adds a `semaphore room = {4}`. This ensures at most four philosophers can attempt to pick up forks, preventing deadlock.
- **Solution using Monitors**: A solution using a monitor is shown, which encapsulates the fork state and logic (`get_forks`, `release_forks`) to manage access.

---
# Concurrency Mechanisms

### UNIX Concurrency Mechanisms
- **Pipes**: A circular buffer allowing two processes to communicate on the producer-consumer model. It is a first-in-first-out queue, written by one process and read by another. Two types: Named and Unnamed.
- **Messages**: A block of bytes with an accompanying type. UNIX provides `msgsnd` and `msgrcv` system calls. Associated with each process is a message queue, which functions like a mailbox.
- **Shared Memory**: A common block of virtual memory shared by multiple processes. Permission is read-only or read-write, determined on a per-process basis. Mutual exclusion must be provided by the processes using the shared memory.
- **Semaphores**: SVR4 uses a generalization of the `semWait` and `semSignal` primitives. Associated with the semaphore are queues of processes blocked on that semaphore.
- **Signals**: A software mechanism that informs a process of the occurrence of asynchronous events. Similar to a hardware interrupt, without priorities.
### Linux Kernel Concurrency Mechanism
Includes all the mechanisms found in UNIX plus:
- **Atomic operations**: Operations that execute without interruption and without interference. Two types: Integer operations and Bitmap operations.
- **Spinlocks**: Only one thread at a time can acquire a spinlock. Any other thread will keep trying (spinning) until it can acquire the lock.
- **Semaphores**: Similar to UNIX SVR4 but also provides an implementation for its own use. Three types of kernel semaphores: Binary, counting, and reader-writer semaphores.
- **Barriers**: To enforce the order in which instructions are executed, Linux provides the memory barrier facility.
### Solaris Thread Synchronization Primitives
In addition to the concurrency mechanisms of UNIX SVR4:
- **Mutual exclusion (mutex) locks**: Used to ensure only one thread at a time can access the resource protected by the mutex. The thread that locks the mutex must be the one that unlocks it.
- **Semaphores**: Solaris provides classic counting semaphores.
- **Readers/writer locks**: Allows multiple threads to have simultaneous read-only access to an object. It also allows a single thread to access the object for writing at one time, while excluding all readers.
- **Condition variables**: Used to wait until a particular condition is true. Condition variables must be used in conjunction with a mutex lock.
### Windows concurrency mechanisms
Important methods of synchronization are:
- **Executive dispatcher objects (using Wait functions)**: The wait functions allow a thread to block its own execution until specified criteria have been met. Dispatcher objects include Events, Mutexes, Semaphores, and Timers.
- **user mode critical sections**: Similar mechanism to mutex except that critical sections can be used only by the threads of a single process. If the system is a multiprocessor, it will attempt to acquire a spin-lock first.
- **slim reader-writer locks**: Added in Windows Vista. A user mode reader-writer lock that enters the kernel to block only after attempting to use a spin-lock. 'Slim' as it normally only requires allocation of a single pointer-sized piece of memory.
- **condition variables**: Added in Windows Vista. The process must declare and initialise a `CONDITION_VARIABLE`. Used with either critical sections or SRW locks.